{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Statistical Hypothesis Testing II\n",
    "\n",
    "In this notebook, we will implement and apply **statistical hypothesis tests** to make inferences about risks of learning algorithms.\n",
    "\n",
    "At the start, we will compare two learning algorithms on one domain via the paired $t$-test.\n",
    "\n",
    "Subsequently, we will compare the two learning algorithm across multiple domains via the Wilcoxon signed-rank test.\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [Paired $t$-test](#paired-t-test)\n",
    "2. [Wilcoxon Signed-rank Test](#wilcoxon-signed-rank-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Paired $t$-test** <a class=\"anchor\" id=\"paired-t-test\"></a>\n",
    "\n",
    "We implement the function [`t_test_paired`](../e2ml/evaluation/_paired_tests.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage. Once, the implementation has been completed, we check it for varying types of tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import t_test_paired\n",
    "r_1 = np.round(stats.norm.rvs(loc=0.1, scale=0.03, size=40, random_state=0), 2)\n",
    "r_2 = np.round(stats.norm.rvs(loc=0.12, scale=0.03, size=40, random_state=1), 2)\n",
    "mu_0 = 0\n",
    "t_statistic, p = t_test_paired(sample_data_1=r_1, sample_data_2=r_2, mu_0=mu_0, test_type=\"right-tail\")\n",
    "assert np.round(t_statistic, 4) == -1.4731 , 'The paired t-test statistic must be ca. -1.4731.' \n",
    "assert np.round(p, 4) == 0.9256, 'The p-value must be ca. 0.9256 for the one-sided right-tail test.' \n",
    "t_statistic, p = t_test_paired(sample_data_1=r_1, sample_data_2=r_2, mu_0=mu_0, test_type=\"left-tail\")\n",
    "assert np.round(t_statistic, 4) == -1.4731 , 'The paired t-test statistic must be ca. -1.4731.' \n",
    "assert np.round(p, 4) == 0.0744, 'The p-value must be ca. 0.0744 for the one-sided left-tail test.' \n",
    "t_statistic, p = t_test_paired(sample_data_1=r_1, sample_data_2=r_2, mu_0=mu_0, test_type=\"two-sided\")\n",
    "assert np.round(t_statistic, 4) == -1.4731 , 'The paired t-test statistic must be ca. -1.4731.' \n",
    "assert np.round(p, 4) == 0.1487, 'The p-value must be ca. 0.1487 for the two-sided test.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to check whether a *support vector classifier* (SVC) significantly outperforms a *Gaussian process classifier* (GPC) on the data set breast cancer, where we use the zero-one loss as performance measure and the paired $t$-test with $\\alpha=0.01$. Design and perform the corresponding evaluation study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough evidence to reject H_0 --> cannot say whether SVC outperforms GPC (nan) (0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Aleyna/PythonProjects/e2ml/e2ml/evaluation/_one_sample_tests.py:90: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  t_statistic = (empirical_mean-mu_0) / np.sqrt((empirical_std**2 / sample_size))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from e2ml.evaluation import cross_validation, zero_one_loss\n",
    "from e2ml.preprocessing import StandardScaler\n",
    "\n",
    "# load data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# create folds\n",
    "sample_indices = np.arange(len(y), dtype=int)\n",
    "n_folds = 8\n",
    "train, test = cross_validation(\n",
    "    sample_indices=sample_indices,\n",
    "    n_folds=n_folds,\n",
    "    y=y,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# result risk list\n",
    "risks_gpc = []\n",
    "risks_svc = []\n",
    "\n",
    "# loop through fodls\n",
    "for train_, test_ in zip(train, test):\n",
    "    # scale data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X[train_])\n",
    "    X_train = scaler.transform(X[train_])\n",
    "    X_test = scaler.transform(X[test_])\n",
    "\n",
    "    # init classifiers\n",
    "    gpc = GaussianProcessClassifier(random_state=0)\n",
    "    svc = SVC(random_state=0)\n",
    "\n",
    "    # train classifiers\n",
    "    gpc.fit(X_train, y[train_])\n",
    "    svc.fit(X_train, y[train_])\n",
    "\n",
    "    # predict on test set\n",
    "    y_gpc = gpc.predict(X[test_])\n",
    "    y_svc = svc.predict(X[test_])\n",
    "\n",
    "    # compute zero-one loss\n",
    "    loss_gpc = zero_one_loss(y_true=y[test_], y_pred=y_gpc)\n",
    "    loss_svc = zero_one_loss(y_true=y[test_], y_pred=y_svc)\n",
    "\n",
    "    # save results of folds\n",
    "    risks_gpc.append(loss_gpc)\n",
    "    risks_svc.append(loss_svc)\n",
    "\n",
    "# compare paired t-test to compare performances\n",
    "alpha = 0.01\n",
    "t_statistic, p = t_test_paired(sample_data_1=risks_gpc, sample_data_2=risks_svc, mu_0=0, test_type='right-tail')\n",
    "\n",
    "if p <= alpha:\n",
    "    print('H_0 is rejected --> SVC outperforms GPC')\n",
    "else:\n",
    "    print(f'Not enough evidence to reject H_0 --> cannot say whether SVC outperforms GPC ({p}) ({alpha})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Questions:**\n",
    "1. (a) What are possible issues of your conducted evaluation study?\n",
    "   \n",
    "   TODO\n",
    "   TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Wilcoxon Signed-rank Test** <a class=\"anchor\" id=\"wilcoxon-signed-rank-test\"></a>\n",
    "\n",
    "We implement the function [`wilcoxon_signed_rank_test`](../e2ml/evaluation/_paired_tests.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage. Once, the implementation has been completed, we check it for varying types of tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import wilcoxon_signed_rank_test\n",
    "\n",
    "# Test for exact computation.\n",
    "r_1 = stats.norm.rvs(loc=0.1, scale=0.03, size=10, random_state=0)\n",
    "r_2 = stats.norm.rvs(loc=0.15, scale=0.03, size=10, random_state=1)\n",
    "d = r_2 - r_1\n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"right-tail\")\n",
    "assert w_statistic == 47 , 'The positive rank sum statistic must be 47.' \n",
    "assert np.round(p, 4) == 0.0244, 'The p-value must be ca. 0.0244 for the one-sided right-tail test.' \n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"left-tail\")\n",
    "assert w_statistic == 47 , 'The positive rank sum statistic must be 47.' \n",
    "assert np.round(p, 4) == 0.9814, 'The p-value must be ca. 0.9814 for the one-sided left-tail test.' \n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"two-sided\")\n",
    "assert w_statistic == 47 , 'The positive rank sum statistic must be 47.' \n",
    "assert np.round(p, 4) == 0.0488, 'The p-value must be ca. 0.0488 for the two-sided test.' \n",
    "\n",
    "# Test for approximative computation.\n",
    "r_1 = stats.norm.rvs(loc=2, scale=0.3, size=100, random_state=0)\n",
    "r_2 = stats.norm.rvs(loc=2.1, scale=0.3, size=100, random_state=1)\n",
    "d = r_2 - r_1\n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"right-tail\")\n",
    "assert w_statistic == 3303 , 'The positive rank sum statistic must be 3303.' \n",
    "assert np.round(p, 4) == 0.0037, 'The p-value must be ca. 0.0037 for the one-sided right-tail test.' \n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"left-tail\")\n",
    "assert w_statistic == 3303 , 'The positive rank sum statistic must be 3303.' \n",
    "assert np.round(p, 4) == 0.9963, 'The p-value must be ca. 0.9963 for the one-sided left-tail test.' \n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=d, test_type=\"two-sided\")\n",
    "assert w_statistic == 3303 , 'The positive rank sum statistic must be 3303.' \n",
    "assert np.round(p, 4) == 0.0075, 'The p-value must be ca. 0.0075 for the two-sided test.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to check whether a *support vector classifier* (SVC) significantly outperforms a *Gaussian process classifier* (GPC) on ten articially generated data sets, where we use the zero-one loss as performance measure and the paired $t$-test with $\\alpha=0.01$. Design and perform the corresponding evaluation study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough evidence to reject H_0 --> cannot say whether SVC outperforms GPC (1.0) (0.01)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create 10 articial data sets.\n",
    "data_sets = []\n",
    "n_classes_list = np.arange(2, 12)\n",
    "for n_classes in n_classes_list:\n",
    "    X, y = make_classification(\n",
    "        n_samples=500, n_classes=n_classes, class_sep=2, n_informative=10, random_state=n_classes\n",
    "    )\n",
    "    data_sets.append((X, y))\n",
    "\n",
    "# create folds\n",
    "sample_indices = np.arange(len(y), dtype=int)\n",
    "n_folds = len(data_sets)\n",
    "train, test = cross_validation(\n",
    "    sample_indices=sample_indices,\n",
    "    n_folds=n_folds,\n",
    "    y=y,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# result risk list\n",
    "risks_gpc = []\n",
    "risks_svc = []\n",
    "\n",
    "# loop through fodls\n",
    "for train_, test_, dataset_ in zip(train, test, data_sets):\n",
    "    # scale data\n",
    "    X, y = dataset_\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X[train_])\n",
    "    X_train = scaler.transform(X[train_])\n",
    "    X_test = scaler.transform(X[test_])\n",
    "\n",
    "    # init classifiers\n",
    "    gpc = GaussianProcessClassifier(random_state=0)\n",
    "    svc = SVC(random_state=0)\n",
    "\n",
    "    # train classifiers\n",
    "    gpc.fit(X_train, y[train_])\n",
    "    svc.fit(X_train, y[train_])\n",
    "\n",
    "    # predict on test set\n",
    "    y_gpc = gpc.predict(X[test_])\n",
    "    y_svc = svc.predict(X[test_])\n",
    "\n",
    "    # compute zero-one loss\n",
    "    loss_gpc = zero_one_loss(y_true=y[test_], y_pred=y_gpc)\n",
    "    loss_svc = zero_one_loss(y_true=y[test_], y_pred=y_svc)\n",
    "\n",
    "    # save results of folds\n",
    "    risks_gpc.append(loss_gpc)\n",
    "    risks_svc.append(loss_svc)\n",
    "\n",
    "# compare paired t-test to compare performances\n",
    "alpha = 0.01\n",
    "w_statistic, p = wilcoxon_signed_rank_test(sample_data_1=risks_gpc, sample_data_2=risks_svc, test_type='right-tail')\n",
    "\n",
    "if p <= alpha:\n",
    "    print('H_0 is rejected --> SVC outperforms GPC')\n",
    "else:\n",
    "    print(f'Not enough evidence to reject H_0 --> cannot say whether SVC outperforms GPC ({p}) ({alpha})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Questions:**\n",
    "2. (a) What are possible issues of your conducted evaluation study?\n",
    "   \n",
    "   TODO\n",
    "   TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
