{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e83585",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Performance Measures  I\n",
    "\n",
    "In this notebook, we will implement **performance measures** for evaluating and comparing classifiers in machine learning. \n",
    "\n",
    "At the start, we will implement a function for computing *confusion matrices*.\n",
    "\n",
    "It serves as a basis for computing the subsequent performance measures with a multi- or single-class focus.\n",
    "\n",
    "Finally, we will compare the implemented performance measures using a simple exemplary classification task.\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [Confusion Matrix](#confusion-matrix)\n",
    "2. [Performances Measures with a Multi-class Focus](#multi-class)\n",
    "3. [Performance Measures with a Single-class Focus](#single-class)\n",
    "4. [Comparison of Performance Measures](#comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f42ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6486326",
   "metadata": {},
   "source": [
    "### **1. Confusion Matrix** <a class=\"anchor\" id=\"confusion-matrix\"></a>\n",
    "\n",
    "The confusion matrix $\\mathbf{C}_\\mathcal{T}(h) \\in \\mathbb{N}^{|\\mathcal{Y}| \\times |\\mathcal{Y}|}$ is a table or matrix that is commonly used to evaluate the performance of a\n",
    "classifier $h: \\mathcal{X} \\rightarrow \\mathcal{Y}$. It summarizes the predictions made by the classifier $h$ on a test set $\\mathcal{T} \\subset \\mathcal{X} \\times \\mathcal{Y}$. The (unormalized) entries of the confusion matrix are defined as:\n",
    "\n",
    "TODO\n",
    "\n",
    "There exist other variants of a confusion matrix, where the entries of the confusion matrix are normalized row-wise, column-wise, or by the total sum of entries. We implement the function [`confusion_matrix`](../e2ml/evaluation/_performance_measures.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage.\n",
    "Once, the implementation has been completed, we check its validity for simple examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import confusion_matrix\n",
    "\n",
    "# Check ranges of class labels.\n",
    "y_1 = [0, -1, 2]\n",
    "y_2 = [0, 1, 2]\n",
    "check = False\n",
    "try:\n",
    "    confusion_matrix(y_true=y_1, y_pred=y_2)\n",
    "except ValueError:\n",
    "    check = True\n",
    "assert check, 'There must be a ValueError because of invalid values.'\n",
    "check = False\n",
    "try:\n",
    "    confusion_matrix(y_true=y_2, y_pred=y_1)\n",
    "except ValueError:\n",
    "    check = True\n",
    "assert check, 'There must be a ValueError because of invalid values.'\n",
    "\n",
    "# Check type of class labels.\n",
    "y_1 = [\"hello\", \"new\", \"test\"]\n",
    "y_2 = [0, 1, 2]\n",
    "check = False\n",
    "try:\n",
    "    confusion_matrix(y_true=y_1, y_pred=y_2)\n",
    "except ValueError:\n",
    "    check = True\n",
    "assert check, 'There must be a TypeError because of invalid value types.'\n",
    "check = False\n",
    "try:\n",
    "    confusion_matrix(y_true=y_2, y_pred=y_1)\n",
    "except ValueError:\n",
    "    check = True\n",
    "assert check, 'There must be a TypeError because of invalid value types.'\n",
    "\n",
    "# Check unequal array lengths.\n",
    "y_1 = [0, -1, 2, 3]\n",
    "y_2 = [0, 1, 2]\n",
    "check = False\n",
    "try:\n",
    "    confusion_matrix(y_true=y_1, y_pred=y_2)\n",
    "except ValueError:\n",
    "    check = True\n",
    "assert check, 'There must be a ValueError because of unqueal array lengths.'\n",
    "check = False\n",
    "try:\n",
    "    confusion_matrix(y_true=y_2, y_pred=y_1)\n",
    "except ValueError:\n",
    "    check = True\n",
    "assert check, 'There must be a ValueError because of unqueal array lengths.'\n",
    "\n",
    "\n",
    "# Test correct computation for various simple examples.\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45055cc",
   "metadata": {},
   "source": [
    "### **2. Performance Measures with a Multi-class Focus** <a class=\"anchor\" id=\"multi-class\"></a>\n",
    "\n",
    "The accuracy $\\mathrm{ACC}_\\mathcal{T}(h) \\in [0, 1]$ of a classifier $h$ on a test set $\\mathcal{T}$ is one of the most know performance measures and can be computed  as the complement of the empirical risk $R_\\mathcal{T}(h)$ or using the confusion matrix $\\mathbf{C}_{\\mathcal{T}}(h)$ as follows:\n",
    "\n",
    "TODO\n",
    "\n",
    "We implement the function [`accuracy`](../e2ml/evaluation/_performance_measures.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage.\n",
    "Once, the implementation has been completed, we check its validity for simple examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b0fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import accuracy\n",
    "\n",
    "# Check unequal array lengths.\n",
    "y_1 = [0, -1, 2, 3]\n",
    "y_2 = [0, 1, 2]\n",
    "check = False\n",
    "try:\n",
    "    print(accuracy(y_true=y_1, y_pred=y_2))\n",
    "except ValueError:\n",
    "    check = True\n",
    "assert check, 'There must be a ValueError because of unqueal array lengths.'\n",
    "check = False\n",
    "try:\n",
    "    accuracy(y_true=y_2, y_pred=y_1)\n",
    "except ValueError:\n",
    "    check = True\n",
    "assert check, 'There must be a ValueError because of unqueal array lengths.'\n",
    "\n",
    "\n",
    "# Test correct computation for various simple examples.\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ceed79",
   "metadata": {},
   "source": [
    "#### **Question:**\n",
    "2. (a) What are limitations of the accuracy as performance measure?\n",
    "\n",
    "   TODO\n",
    "\n",
    "Cohenâ€™s $\\kappa$ represents a more realistic estimate of classifier effectiveness, which is the proportion of labels that the classifier gets right over and above chance agreement. We can compute this performance measure according to:\n",
    "\n",
    "TODO\n",
    "\n",
    "We implement the function [`cohen_kappa_score`](../e2ml/evaluation/_performance_measures.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage. Once, the implementation has been completed, we check its validity for simple examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import cohen_kappa\n",
    "\n",
    "# Test correct computation for various simple examples.\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd40eb9",
   "metadata": {},
   "source": [
    "### **3. Performance Measures with a Single-class Focus** <a class=\"anchor\" id=\"single-class\"></a>\n",
    "\n",
    "The F measure combines precision and recall in a score by computing the weighted harmonic mean of both. For any $\\alpha \\in \\mathbb{R}_{>0}$, the F measure can be given as:\n",
    "\n",
    "TODO\n",
    "\n",
    "The F1 measure or balance F measure weights the recall and precision of the classifier evenly via $\\alpha=1$. The macro F1 measure is an extension toward multi-class problems. Its idea is to compute the F1 score for each class and then taking the arithmetic mean of these scores.\n",
    "\n",
    "We implement the function [`macro_f1_measure`](../e2ml/evaluation/_performance_metrics.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage. Once, the implementation has been completed, we check its validity for simple examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9554ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2ml.evaluation import macro_f1_measure\n",
    "\n",
    "# Test correct computation for various simple examples.\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e6833",
   "metadata": {},
   "source": [
    "### **4. Comparison of Performance Measures** <a class=\"anchor\" id=\"comparison\"></a>\n",
    "In the following, we perform an exemplary evaluation study to compare the performance measures accuracy, Cohen's kappa, and macro F1. Therefore, we fit a logistic regression model on a synthetic data set and compute the corresponding measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ec47e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Generate classification dataset.\n",
    "X, y = make_blobs(n_samples=[20, 50, 400], random_state=0)\n",
    "\n",
    "# Visualize the dataset.\n",
    "# TODO\n",
    "\n",
    "# Split the dataset into 80% training data and 20% test data.\n",
    "# TODO\n",
    "\n",
    "# Fit a logistic regression model on the training data.\n",
    "lr = LogisticRegression(max_iter=2000, random_state=0) # <- SOLUTION\n",
    "lr.fit(X[train], y[train]) # <- SOLUTION\n",
    "\n",
    "# Evaluate and print the three performance measures on the training and test set.\n",
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
