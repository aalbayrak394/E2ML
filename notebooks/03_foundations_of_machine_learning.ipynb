{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39c5f71d",
   "metadata": {},
   "source": [
    "# Foundations of Machine Learning\n",
    "\n",
    "In this notebook, we will learn about the basics of machine learning. \n",
    "\n",
    "At the start, we will implement various **loss and risk** estimation functions.\n",
    "\n",
    "Subsequently, we will implement a **binary logistic regression** (BLR) model according to the [scikit-learn guidelines](https://scikit-learn.org/stable/developers/develop.html).\n",
    "\n",
    "Further, we will introduce **nonlinear basis functions** as a prerequisite for nonlinear decision boundaries when using BLR models.\n",
    "\n",
    "Finally, we apply BLR models to an artificial data set to investigate the **under- and overfitting** issue.\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [Loss Functions and Notions of Risk](#loss-functions-and-notions-of-risk)\n",
    "2. [Binary Logistic Regression](#binary-logistic-regression)\n",
    "3. [Nonlinear Basis Functions](#basis-functions)\n",
    "4. [Under- and Overfitting](#under-overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a714745d-6c84-452f-b796-681088b60161",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from math import isclose\n",
    "from ipywidgets import interactive, IntSlider, FloatSlider"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e2e7e46",
   "metadata": {},
   "source": [
    "### **1. Loss Functions and Notions of Risk** <a class=\"anchor\" id=\"loss-functions-and-notions-of-risk\"></a>\n",
    "\n",
    "The most popular loss function in classification is the zero-one loss function $L_{0/1}$, which is given by\n",
    "\n",
    "$\n",
    "L_{0/1}(y, y') = \\delta(y \\neq y') = \n",
    "\\begin{cases} \n",
    "1 & if & y \\neq y' \\\\\n",
    "0 & if & y = y'\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "#### **Question:**\n",
    "1. (a) What are the reasons that the zero-one loss function $L_{0/1}$ is typically used for assessing a classifier but not for training?\n",
    "\n",
    "   The Gradient Descent Algorithm used in training cannot be applied if the loss is non-convex or non-smooth \n",
    "   \n",
    "Alternative loss functions have been proposed that are suitable for optimizing a classifier during training. One of such functions is the *binary cross entropy* (BCE) loss function $L_\\mathrm{BCE}$, which is given by:\n",
    "\n",
    "$ L_{BCE}(p, p') = -p \\cdot ln(p') - (1-p) \\cdot ln(1-p') \\geq 0$\n",
    "\n",
    "#### **Question:**\n",
    "1. (b) Is the BCE loss function convex with respect to the estimated probability? Prove your answer.\n",
    "\n",
    "   *Remark: A function $f$ is convex in an interval $\\mathcal{I}$, if $\\forall x \\in \\mathcal{I}: f^{\\prime\\prime}(x) \\geq 0$.*\n",
    "\n",
    "   first derivative: $ - \\dfrac{p'-p}{(p'-1)p} $ \n",
    "   \n",
    "   second derivative: $ \\dfrac{p'^2-2pp'+p}{\\left(p'-1\\right)^2p'^2} $\n",
    "\n",
    "   For $p' \\geq 0$ the second derivative is always $\\geq 0$ \n",
    "   \n",
    "For a given loss function $L$ and a dataset $\\mathcal{D}$, the empirical risk $R_{\\mathcal{D}}(h)$ of a classifier $h$ is given by:\n",
    "\n",
    "$\n",
    "R_{\\mathcal{D}}(h) = \\dfrac{1}{\\mathcal{D}} \\sum_{(x,y) \\in \\mathcal{D}} L(y, h(x))\n",
    "$\n",
    "\n",
    "With this knowledge, we implement the empirical risk for the zero-one loss function $L_{0/1}$ and the BCE loss function $L_{\\mathrm{BCE}}$ in the Python file [`e2ml.evaluation._loss_functions`](../e2ml/evaluation/_loss_functions.py). Afterwards, we check our code by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8377f885",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "There must be a ValueError because of invalid values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     check \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[39massert\u001b[39;00m check, \u001b[39m'\u001b[39m\u001b[39mThere must be a ValueError because of invalid values.\u001b[39m\u001b[39m'\u001b[39m \n",
      "\u001b[0;31mAssertionError\u001b[0m: There must be a ValueError because of invalid values."
     ]
    }
   ],
   "source": [
    "from e2ml.evaluation import zero_one_loss, binary_cross_entropy_loss\n",
    "\n",
    "y_true = [0, 1, 0, 1, 0]\n",
    "y_pred = [0, 0, 0, 1, 0]\n",
    "check = zero_one_loss(y_true, y_pred) == 0.2\n",
    "assert check, 'The empirical risk must be 0.2 for this data set.' \n",
    "\n",
    "y_true = [0, 1, 0, 1, 0]\n",
    "y_pred = [0, 0.9, 0.1, 0.5, 0.5]\n",
    "check = isclose(binary_cross_entropy_loss(y_true, y_pred), 0.319, abs_tol=0.001)\n",
    "assert check, 'The empirical risk must be around 0.319 for this data set.' \n",
    "\n",
    "y_true_list = [[2], [-1], [0.5], [0.5]]\n",
    "y_pred_list = [[0.5], [0.5], [2], [-1]]\n",
    "for y_true, y_pred in zip(y_true_list, y_pred_list):\n",
    "    check = False\n",
    "    try:\n",
    "        binary_cross_entropy_loss(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        check = True\n",
    "    assert check, 'There must be a ValueError because of invalid values.' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca7473c8",
   "metadata": {},
   "source": [
    "### **2. Binary Logistic Regression** <a class=\"anchor\" id=\"binary-logistic-regression\"></a>\n",
    "\n",
    "*Binary logistic regression* (BLR) is a space of classifiers optimizing the BCE loss function. Instead of a class label, a BLR model with weights $\\mathbf{w} \\in \\mathbb{R}^M$ and $\\boldsymbol{\\phi}: \\mathbb{R}^D \\rightarrow \\mathbb{R}^M$ as a vector of $M$ basis functions estimates the conditional probability according to:\n",
    "\n",
    "    s. Folie 28\n",
    "\n",
    "For a *regularized risk minimization* (RRM) algorithm, we train a BLR model by optimizing\n",
    "\n",
    "    s. Folie 29 : optimize the empirical risk for all possible parameters for a basis function\n",
    "    -> find optimal parameters\n",
    "\n",
    "For optimization, we could use a gradient descent approach:\n",
    "\n",
    "$$\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\cdot \\nabla\\left(R_\\mathcal{D}(\\mathbf{w}_t) + J(h_\\mathbf{w})\\right),$$\n",
    "\n",
    "where $\\nabla$ denotes the gradient, $t$ the current optimization step, and $\\eta \\in \\mathbb{R}_{>0}$ the learning rate defining the step size in each optimization step. The initial weights $\\mathbf{w}_0$ can be chosen randomly and the subsequent optimization steps can be repeated until a stopping criterion is reached, e.g., maximum number of steps. The gradient $\\nabla\\left(R_\\mathcal{D}(\\mathbf{w}_t) + J(h_\\mathbf{w})\\right)$ with respect to the weights $\\mathbf{w}$ is then given by:\n",
    "\n",
    "TODO\n",
    "\n",
    "With this knowledge, we implement the class [`BinaryLogisticRegression`](../e2ml/models/_binary_logistic_regression.py) in the [`e2ml.models`](../e2ml/models) subpackage. However, there we use [L-BFGS-B](https://en.wikipedia.org/wiki/Limited-memory_BFGS) as a so-called quasi-Netwon algorithm, which is an advancement of the standard gradient descent algorithm. Once, the implementation has been completed, we check its validity on a simple two-dimensional artificial dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a27f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from e2ml.models import BinaryLogisticRegression\n",
    "\n",
    "# Create linearly separable dataset.\n",
    "X, y = make_blobs(n_samples=100, centers=[[-1, -1], [1, 1]], cluster_std=0.1, random_state=0)\n",
    "\n",
    "# Fit `BinaryLogisticRegression` with `maxiter=1` and `lmbda=0` on the artificial data set.\n",
    "model = BinaryLogisticRegression(maxiter=1, lmbda=0)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions using the fitted `BinaryLogisticRegression` model on the training set `X`.\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Compute empirical risk as `risk` with the zero-one loss function on the training set `X`.\n",
    "risk = zero_one_loss(y_true=y, y_pred=y_pred)\n",
    "\n",
    "assert risk == 0, 'The classifier must reach an accuracy of 1.0 for this data set.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5f6295",
   "metadata": {},
   "source": [
    "### **3. Nonlinear Basis Functions** <a class=\"anchor\" id=\"basis-functions\"></a>\n",
    "\n",
    "For certain learning tasks, it is useful to add complexity to a machine learning model by considering\n",
    "nonlinear transformations of the samples. \n",
    "\n",
    "**Definition 3.12** <font color='red'>**Nonlinear Basis Functions**</font> \n",
    "\n",
    "Nonlinear basis functions $\\boldsymbol{\\phi}: \\mathbb{R}^D \\rightarrow \\mathbb{R}^M$ nonlinearly transform a $D$-dimensional instance $\\mathbf{x} \\in \\mathbb{R}^D$ to an $M$-dimensional features space $\\mathbb{R}^M$ according to:\n",
    "\n",
    "$$\\boldsymbol{\\phi}(\\mathbf{x}) = \\begin{bmatrix} \\phi_1(\\mathbf{x}) \\\\ \\vdots \\\\ \\phi_M(\\mathbf{x}) \\end{bmatrix}.$$\n",
    "\n",
    "**Remark:** With nonlinear basis functions, a (generalized) linear machine learning model, e.g., BLR, can output nonlinear outputs, e.g., decision boundaries.\n",
    "\n",
    "**Examples:** Two popular types of nonlinear basis functions are: \n",
    "- *multivariate polynomials* with a user-defined maximum degree, e.g., for a degree of two and a two-dimensional instance, we get\n",
    "\n",
    "  $$\\boldsymbol{\\phi}(\\mathbf{x}) = \\boldsymbol{\\phi}\\left(\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\right) = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ x_1 \\cdot x_2 \\\\ x_1^2 \\\\ x_2^2 \\end{bmatrix},$$\n",
    "  \n",
    "- and *Gaussian basis functions* of the form\n",
    "\n",
    "  $$\\phi_j(\\mathbf{x}) = \\exp\\left(\\frac{-||\\mathbf{x} - \\boldsymbol{\\mu}_j||_2^2}{2 \\sigma_j^2}\\right),$$\n",
    "\n",
    "  where $\\boldsymbol{\\mu} \\in \\mathbb{R}^D$ controls the position of the $j$-th basis function and the parameter $\\sigma_j \\in \\mathbb{R}_>0$ its spatial extension.\n",
    "  \n",
    "In the following, we exemplary illustrate the use of Gaussian basis functions. For this purpose, we want to define two basis functions such that the transformed instances define a linearly separable training set, i.e., it should be possible to draw a single straight line separating the blue from the red instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5155a49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a20e1ac55064204a9cd4311cfd9ec3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='mu_11', max=10.0, min=-10.0), FloatSlider(value=0.0,…"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def visualize_gaussian_basis_functions(mu_11, mu_12, mu_21, mu_22, sigma_1, sigma_2):\n",
    "    \"\"\"\n",
    "    Visualizes the application of Gaussian basis functions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mu_11 : float\n",
    "        First value of mu_1 of the first basis function.\n",
    "    mu_12 : float\n",
    "        Second value of mu_1 of the first basis function.\n",
    "    mu_21 : float\n",
    "        First value of mu_2 of the second basis function.\n",
    "    mu_22 : float\n",
    "        Second value of mu_2 of the second basis function.\n",
    "    sigma_1 : positive float\n",
    "        Spatial extension of the first basis function.\n",
    "    sigma_2 : positive float\n",
    "        Spatial extension of the second basis function.\n",
    "    \"\"\"\n",
    "    X, y = make_blobs(n_samples=50, centers=4, random_state=7, cluster_std=0.75)\n",
    "    y %= 2\n",
    "    \n",
    "    # Define array of `mus`.\n",
    "    mus = np.array([[mu_11, mu_12], [mu_21, mu_22]])\n",
    "    \n",
    "    # Define array of `sigmas`.\n",
    "    sigmas = np.array([sigma_1, sigma_2])\n",
    "    \n",
    "    # Compute transformation via Gaussian basis functions.\n",
    "    Phi = np.exp(-euclidean_distances(X, mus)/ 2*sigmas**2)\n",
    "    \n",
    "    # Plot results.\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=40, alpha=0.3)\n",
    "    plt.scatter(mus[:, 0], mus[:, 1], c=\"green\", s=120, marker=\"x\", label=\"$(\\mu_1, \\mu_2)^\\mathrm{T}$\")\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(Phi[:, 0], Phi[:, 1], c=y, cmap='coolwarm', s=40, alpha=0.3)\n",
    "    plt.xlabel(\"$\\phi_1$\")\n",
    "    plt.ylabel(\"$\\phi_2$\")\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_gaussian_basis_functions, \n",
    "    mu_11=FloatSlider(value=0, min=-10, max=10),\n",
    "    mu_12=FloatSlider(value=0, min=-10, max=10),\n",
    "    mu_21=FloatSlider(value=0, min=-10, max=10),\n",
    "    mu_22=FloatSlider(value=0, min=-10, max=10),\n",
    "    sigma_1=FloatSlider(value=1, min=0.1, max=10),\n",
    "    sigma_2=FloatSlider(value=1, min=0.1, max=10),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58147427",
   "metadata": {},
   "source": [
    "#### **Questions:**\n",
    "3. (a) What is a suitable parametrization of the above basis functions for obtaining a linearly separable transformation?\n",
    "\n",
    "   TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595067bd",
   "metadata": {},
   "source": [
    "### **4. Under- and Overfitting** <a class=\"anchor\" id=\"under-overfitting\"></a>\n",
    "\n",
    "For our two-dimensional data set, we want to visualize the BLR models estimated decision boundaries including the estimated class-membership probabilities.\n",
    "For this purpose, we implement the function [`plot_decision_boundary`](../e2ml/evaluation/_visualization.py) in the [`e2ml.evaluation`](../e2ml/evaluation) subpackage.\n",
    "\n",
    "Given this function, we create an interactive visualization in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b5c3d40",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d0a8f97a5a478b997592565006b12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1000, description='maxiter', max=10000), FloatSlider(value=0.0, descript…"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from e2ml.evaluation import plot_decision_boundary\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def visualize_blr(maxiter, lmbda, degree, train_ratio, random_state):\n",
    "    \"\"\"\n",
    "    Visualize decision boundary and estimated conditional class probabilities for a BLR model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    maxiter : positive int\n",
    "        Number of maximum optimization iterations.\n",
    "    lmbda : non-negative float\n",
    "        Regularization rate used by the RRM algorithm.\n",
    "    degree : non-negative int\n",
    "        Maximum degree of the multivariate polynomial.\n",
    "    train_ratio : float in (0, 1)\n",
    "        Ratio of training samples.\n",
    "    \"\"\"\n",
    "    # Generate dataset.\n",
    "    X, y = make_classification(\n",
    "        n_samples=100,\n",
    "        n_features=2,\n",
    "        n_redundant=0,\n",
    "        flip_y=0.25,\n",
    "        random_state=100,\n",
    "        scale=0.1,\n",
    "        class_sep=1.5,\n",
    "    )\n",
    "    \n",
    "    # Get boundaries of the feature space.\n",
    "    bound=[[X[:, 0].min()-0.2, X[:, 1].min()-0.2], [X[:, 0].max()+0.2, X[:, 1].max()+0.2]]\n",
    "    \n",
    "    # Randomly split train and test data by creating the index arrays `train_idx` and `test_idx`.\n",
    "    indices = np.arange(100)\n",
    "    np.random.shuffle(indices)\n",
    "    split_point = int(train_ratio*100)\n",
    "\n",
    "    train_idx = indices[:split_point]\n",
    "    test_idx = indices[split_point:]\n",
    "    \n",
    "    # Create polynomial feature transformation `poly` with `degree`.\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    \n",
    "    # Create `blr` model with `n_optimization_steps` and `lmbda`.\n",
    "    blr = BinaryLogisticRegression(maxiter, lmbda)\n",
    "    \n",
    "    # Create pipeline.\n",
    "    clf = make_pipeline(poly, blr)\n",
    "    \n",
    "    # Fit `BinaryLogisticRegression` as `clf` on training data.\n",
    "    clf.fit(X=X[train_idx], y=y[train_idx])\n",
    "    \n",
    "    # Compute zero-one loss on train and test data, i.e., `train_error` and `test_error`.\n",
    "    train_error = zero_one_loss(y[train_idx], clf.predict(X[train_idx]))\n",
    "    test_error = zero_one_loss(y[test_idx], clf.predict(X[test_idx]))\n",
    "    \n",
    "    # Plot results.\n",
    "    plot_decision_boundary(clf=clf, bound=bound)\n",
    "    plt.scatter(\n",
    "        X[train_idx, 0],\n",
    "        X[train_idx, 1],\n",
    "        marker='o',\n",
    "        label='Training Samples',\n",
    "        facecolors='none',\n",
    "        edgecolors='k',\n",
    "        linewidth=4, \n",
    "        s=60\n",
    "    )\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=40)\n",
    "    plt.title(f'Train Error: {round(train_error, 3)}, Test Error: {round(test_error, 3)}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "interactive(\n",
    "    visualize_blr, \n",
    "    maxiter=IntSlider(value=1000, min=0, max=10000), \n",
    "    eta=FloatSlider(value=1, min=0, max=10), \n",
    "    lmbda=FloatSlider(value=0.0, min=0.0, max=10, step=0.001),\n",
    "    degree=IntSlider(value=1, min=1, max=10), \n",
    "    train_ratio=FloatSlider(value=0.5, min=0.02, max=0.98, step=0.01),\n",
    "    random_state=IntSlider(value=0, min=0, max=10),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8a7363d",
   "metadata": {},
   "source": [
    "The interactive visualization allows us to study the effect of the two parameters `lmbda` and `degree`. In particular, we can study the phonemes of under- and overfitting to answer the following questions:\n",
    "\n",
    "#### Questions:\n",
    "4. (a) How does the regularization rate `lmbda` affect the BLR model's decision boundary and its estimated class-membership probabilities? How is it related to over- and underfitting?\n",
    "\n",
    "   higher lambda: simpler decision boundary, more generelized model\\\n",
    "   smaller lambda: more complex decision boundary, might fit better on training data\n",
    "\n",
    "   overfitting can occur when lambda is too small leading to a model which is specific for the training data and might perform poorly on unseen data \\\n",
    "   underfitting can occur when lambda is too high leading to a too simple model which is not flexible enough (even on the training dataset)\n",
    "   \n",
    "   (b) How does the `degree` of the polynomial affect the BLR model's decision boundary and its estimated class-membership probabilities? How is it related to over- and underfitting?\n",
    "\n",
    "   The higher the degree, the more complex the model gets. A too high degree tends to cause overfitting whereas a too small degree tends to cause underfitting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
